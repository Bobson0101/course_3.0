{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Signal Classification"
      ],
      "metadata": {
        "id": "82dhsvRUKofa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "訊號與圖的分類一樣，在preprocess後可使用神經網路做一些AI任務，例如訊號分類、迴歸還有生成等等。\n",
        "\n",
        "這個部分我們用音訊作為訊號的範例，來試著將聲音訊號做分類，包含以下部分:\n",
        "- Audio Data Loader\n",
        "- RNN audio classification\n",
        "- CNN audio classification"
      ],
      "metadata": {
        "id": "5mo7TK6mKrry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "開始之前我們先準備一些內容。\n",
        "\n",
        "我們使用的範例資料集是tensorflow提供的[Mini Speech Commands](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html)資料集，從官網下載。"
      ],
      "metadata": {
        "id": "R-B1OWAKMDtL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTGJO2yTGgQ_"
      },
      "outputs": [],
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\n",
        "!unzip mini_speech_commands.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import IPython.display as idp # 播音工具\n",
        "import librosa.display as ldp # 畫頻譜圖工具\n",
        "import numpy as np # 輔助運算\n",
        "import matplotlib.pyplot as plt # 輔助畫圖\n",
        "\n",
        "# import model用到的內容\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "g1m6erWYMR6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio Data Loader"
      ],
      "metadata": {
        "id": "jo_9jVjhRVfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "與前面CNN相同，需要有data loader去對資料做讀取，而tensorflow沒有原生讀音訊的data loader (TF 2.11有，但在Colab上要另外灌TF2.11)，所以這部分要自己寫。"
      ],
      "metadata": {
        "id": "ryzrWNvWRZqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 讀取音訊檔及資料切分"
      ],
      "metadata": {
        "id": "KPMvRiOT5uiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob #拿來列資料夾內容的小套件\n",
        "from sklearn.model_selection import train_test_split # 切分資料集用\n",
        "\n",
        "def find_class(x):\n",
        "    # 根據格式，找到所屬class\n",
        "    return x.strip('/')[-2]\n",
        "\n",
        "def audio_folder_datasets( dataset_path,class_dictionary, sr=22050, duration=None):\n",
        "    # 輸入: \n",
        "    ## dataset_path - 資料夾，內有數個不同class的資料夾，內有.wav檔\n",
        "    ## class_dictionary - dictionary物件，對應每個資料夾的class\n",
        "    ## sr -  讀取的sampling rate\n",
        "    ## duration - 可指定秒數(float32)，不指定則為原檔長度\n",
        "    file_names = []\n",
        "    labels = []\n",
        "    for cls, class_id in class_dictionary.items():\n",
        "        f_list=glob(dataset_path+f'{cls}/*.wav') # 找到該class的所有檔案\n",
        "        file_names.extend(f_list) # 加入列表\n",
        "        labels.extend([class_id]*len(f_list)) # 加入相應labels\n",
        "    print(\"total:\",f\"{len(file_names)} files of {len(class_dictionary)} classes\")\n",
        "    def load_wav(fname):\n",
        "        # 使用指定sampling rate, duration讀檔\n",
        "        # 會從binary格式轉成一般文字再開始做\n",
        "        return librosa.load(fname.numpy(), sr=sr, duration=duration)\n",
        "    def load_file(fname):\n",
        "        # 使用pyfunction將loading function用在tensorflow tnesor上\n",
        "        return tf.py_function(func=load_wav, inp=[fname], Tout=tf.float32)\n",
        "\n",
        "    def get_dataset(paths_,labels_):\n",
        "        # 得到所有資料夾名稱\n",
        "        path_ds = tf.data.Dataset.from_tensor_slices(paths_) # 轉換成檔名的Dataset物件\n",
        "        label_ds= tf.data.Dataset.from_tensor_slices(labels_)\n",
        "\n",
        "        data_ds = path_ds.map(\n",
        "                load_file,\n",
        "                num_parallel_calls=tf.data.AUTOTUNE,\n",
        "            )\n",
        "        return tf.data.Dataset.zip((data_ds,label_ds))\n",
        "\n",
        "    fname_train, fname_val, label_train, label_val = train_test_split(file_names,labels,test_size=0.2)\n",
        "    return get_dataset(fname_train,label_train), get_dataset(fname_val,label_val)\n",
        "    "
      ],
      "metadata": {
        "id": "m3KXFuxQRZD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "生成一個dataset拿來用作基礎。"
      ],
      "metadata": {
        "id": "DfRAtBnZ2ZIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 準備一些參數輸入進function生成dataset\n",
        "DATASET_PATH='mini_speech_commands/'\n",
        "class_dict={\n",
        "    'down':0,\n",
        "    'go':1,\n",
        "    'left':2,\n",
        "    'no':3,\n",
        "    'right':4,\n",
        "    'stop':5,\n",
        "    'up':6,\n",
        "    'yes':7\n",
        "}\n",
        "SR=22050\n",
        "DURATION=0.8\n",
        "tran_ds, val_ds = audio_folder_datasets(DATASET_PATH,class_dict,sr=SR,duration=DURATION)"
      ],
      "metadata": {
        "id": "Yow6YjV7eqcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "觀察data基本性質，一次丟出一個signal以及一個label"
      ],
      "metadata": {
        "id": "iRVQN_8r3tZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in tran_ds:\n",
        "    print(x.shape,x.numpy().min(),x.numpy().max())\n",
        "    print(y)\n",
        "    break\n",
        "# 畫出來\n",
        "ldp.waveplot(x.numpy(),sr=SR)\n",
        "\n",
        "# 聽看看\n",
        "idp.Audio(x.numpy(),rate=SR)"
      ],
      "metadata": {
        "id": "jzc53mLj2Q4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "畫出一個例子"
      ],
      "metadata": {
        "id": "SDrINt5U3xh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 輸入NN前做轉換"
      ],
      "metadata": {
        "id": "hb0sjjxy50he"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在預處裡時使用librosa會很慢，幸好與```librosa.stft```類似，可以使用```tensorflow.signal.stft```做時頻分析，在操作的各種過程中只能用tf function來操作。\n",
        "\n",
        "其axis為[time,frequency]，與librosa相反，使用```librosa.specshow```觀察時記得要做transpose。\n",
        "\n",
        "為什麼要反過來是為了配合RNN的預設axis [batch,time,...]，將time擺在batch後面第一位。\n",
        "\n",
        "若希望使用CNN模型，記得在最後多加一個空的axis，因為CNN適用的axis是[batch, hight, width, channels]。"
      ],
      "metadata": {
        "id": "5IZGg4NvV1s-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N=512\n",
        "H=128\n",
        "def get_stft(waveform):\n",
        "  # 做STFT (用tensorflow得比較快)\n",
        "  spectrogram = tf.signal.stft(\n",
        "      waveform, frame_length=N, frame_step=H)\n",
        "  # 這邊frame_length是librosa的n_fft\n",
        "  #     frame_step是librosa的hop_length\n",
        "  # 使用tf.signal stft出來時，單位為((timepoints-n_fft)/hop_length, n_fft/2)\n",
        "  # 這是為了配合RNN等模型的\n",
        "  \n",
        "  # 取magnitude\n",
        "  spectrogram = tf.abs(spectrogram)\n",
        "\n",
        "  # 若是多加一個維度，可以用於CNN，shape (`batch_size`, `height`, `width`, `channels`).\n",
        "  # spectrogram = spectrogram[..., tf.newaxis]\n",
        "  return spectrogram\n",
        "\n",
        "# 使用STFT當作preprocess function\n",
        "tran_ds_stft= tran_ds.map(lambda x,y: (get_stft(x), y), num_parallel_calls=tf.data.AUTOTUNE).cache().shuffle(6400).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds_stft= val_ds.map(lambda x,y: (get_stft(x), y), num_parallel_calls=tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "jLi5zn7a3NQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 預讀資料，放進GPU\n",
        "for x_S,y in tran_ds_stft:\n",
        "    pass"
      ],
      "metadata": {
        "id": "VxHlfYUynEEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 可以看一下資料\n",
        "for x_S,y in val_ds_stft:\n",
        "    print(x_S.shape,x_S.numpy().min(),x_S.numpy().max())\n",
        "    print(y)\n",
        "    break\n",
        "plt.figure(figsize=(6,5))\n",
        "ldp.specshow(x_S.numpy().T,sr=SR,x_axis=\"s\",y_axis=\"hz\",cmap=\"jet\") # 記得做transpose\n",
        "plt.colorbar(format=\"%+4.f\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CBWPxPyWW08V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 跟librosa差不多，librosa有做time padding，會長一些\n",
        "x_S_=librosa.stft(x.numpy(),n_fft=N,hop_length=H)\n",
        "plt.figure(figsize=(6,5))\n",
        "ldp.specshow(abs(x_S_),sr=SR,x_axis=\"s\",y_axis=\"hz\",cmap=\"jet\")\n",
        "plt.colorbar(format=\"%+4.f\")\n",
        "plt.show()\n",
        "x_S_.shape"
      ],
      "metadata": {
        "id": "fhfraNwUTo6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN Audio classifcation"
      ],
      "metadata": {
        "id": "4Im2kjcGmFSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "我們可使用RNN來做對剛剛的頻譜作classification的訓練"
      ],
      "metadata": {
        "id": "UyWP60TbmKoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for example_spectrograms, example_spect_labels in tran_ds_stft.take(1):\n",
        "  break"
      ],
      "metadata": {
        "id": "je3BLdd3mB31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = example_spectrograms.shape[1:]\n"
      ],
      "metadata": {
        "id": "PKq0-Oc0niuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models"
      ],
      "metadata": {
        "id": "krVVzebD5q2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference\n",
        "* TF官網教學: https://www.tensorflow.org/tutorials/audio/simple_audio"
      ],
      "metadata": {
        "id": "x1oWIACng2wH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B8csPV1HhNBk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}